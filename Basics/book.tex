\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{url}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}

\title{NeuroIncept Decoder for High-Fidelity Speech Reconstruction from Neural Activity}
\author{Owais Mujtaba Khanday \and José L. Pérez-Córdoba \and Mohd Yaqub Mir \and Ashfaq Ahmad Najar \and Jose A. Gonzalez-Lopez}
\date{\today}

\begin{document}

\frontmatter
\maketitle

\chapter{Abstract}
This paper introduces a novel algorithm designed for speech synthesis from neural activity recordings obtained using invasive electroencephalography (EEG) techniques. The proposed system offers a promising communication solution for individuals with severe speech impairments. Central to our approach is the integration of time-frequency features in the high-gamma band computed from EEG recordings with an advanced NeuroIncept Decoder architecture. This neural network architecture combines Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to reconstruct audio spectrograms from neural patterns. Our model demonstrates robust mean correlation coefficients between predicted and actual spectrograms, though inter-subject variability indicates distinct neural processing mechanisms among participants. Overall, our study highlights the potential of neural decoding techniques to restore communicative abilities in individuals with speech disorders and paves the way for future advancements in brain-computer interface technologies.

\section*{Keywords}
Brain-computer interfaces, speech synthesis, deep neural networks, EEG.

\chapter{Acknowledgements}
This work was supported by grant PID2022-141378OB-C22 funded by MICIU/AEI/10.13039/501100011033 and by ERDF/EU.

\tableofcontents
\listoffigures

\mainmatter

\chapter{Introduction}
Speech is one of the most fundamental yet complex human abilities, serving as the primary medium for communication, social interaction, and the expression of thought. However, for millions of people worldwide, the ability to produce intelligible speech is compromised by a variety of neurological and physiological conditions. The inability to communicate effectively not only hinders social integration but also leads to severe frustration, isolation, and a significant decline in the quality of life. This chapter provides a comprehensive overview of the challenges posed by speech disorders, the technological evolution that has led to the development of brain-computer interfaces (BCIs), and the specific motivations behind the NeuroIncept Decoder.

\section{Background on Speech Disorders}
Speech disorders encompass a wide range of conditions that impair the ability to produce sounds, form words, or maintain the natural rhythm and flow of speech. These impairments often result from damage to the intricate neural networks in the brain responsible for motor control and language processing, or from physical damage to the vocal apparatus, including the muscles, nerves, and structures such as the larynx and tongue \cite{jullien2021screening, Gonzalez2020}.

Data from the National Institute on Deafness and Other Communication Disorders (NIDCD) indicates that speech and language disorders are alarmingly prevalent. Approximately 7.7\% of children in the United States aged 3 to 17 have experienced a disorder related to voice, speech, language, or swallowing in the past year \cite{bib1}. Among younger children (ages 3 to 6), this figure rises to nearly 11\% \cite{bib2}. Global statistics mirror these trends, highlighting a significant public health challenge that spans all age groups and demographics.

The etiology of speech disorders is diverse. Conditions such as stuttering and apraxia of speech often manifest in childhood, while aphasia and dysarthria are more common in adults, frequently occurring as a result of stroke, traumatic brain injury (TBI), or neurodegenerative diseases. For individuals with amyotrophic lateral sclerosis (ALS), the progressive loss of motor neurons eventually leads to total "locked-in" syndrome (LIS), where cognitive functions remain entirely intact but the individual is unable to move any voluntary muscles, including those required for speech \cite{neuro1}. In such cases, traditional augmentative and alternative communication (AAC) devices—such as eye-trackers or puff-and-sip switches—often prove insufficient for high-speed, natural communication. This creates an urgent clinical need for neuroprosthetic solutions that can bypass the paralyzed peripheral nerves and muscles by decoding intentions directly from the brain.

\section{Evolution of Brain-Computer Interfaces}
The field of Brain-Computer Interfaces (BCIs) has undergone a revolutionary transformation since the first recordings of human brain activity. Historically, BCIs were designed primarily to restore basic motor functions, such as controlling a cursor on a screen or a robotic arm, using binary or low-dimensional control signals. However, the ultimate goal of BCI research has always been the restoration of high-bandwidth communication.

Early BCIs relied heavily on non-invasive electroencephalography (EEG), which records electrical activity from the scalp. While EEG is safe and easily deployable, it suffers from poor spatial resolution and a low signal-to-noise ratio (SNR) because the skull and scalp act as low-pass filters, blurring the high-frequency neural signals that carry information about articulation and prosody.

The shift toward invasive neural recording techniques, such as electrocorticography (ECoG) and stereotactic EEG (sEEG), marked a turning point in the field \cite{neuro2, neuro3}. By placing electrodes directly on or within the brain tissue, researchers gained access to the high-gamma frequency band (70-170 Hz), which is closely associated with local neural population activity and has proven to be essential for decoding the complex motor and linguistic features of speech \cite{band1, band2}. These invasive systems have demonstrated the ability to decode speech at speeds approaching natural conversation, moving from simple word recognition to the synthesis of full sentences.

\section{Invasive vs. Non-Invasive Neural Recording}
The choice of neural recording modality is a critical factor in the design of a speech neuroprosthesis. Non-invasive EEG, while valuable for applications like sleep study or basic motor control, is hindered by the volume conduction effect, where electrical signals from different brain regions overlap as they travel through the skull. This makes it extremely difficult to isolate the fine-grained activities of the motor cortex responsible for the rapid movements of the vocal tract during speech.

In contrast, invasive modalities like sEEG provide high-fidelity, localized recordings with millisecond-level temporal resolution. Stereotactic EEG involves the implantation of depth electrodes that can reach deep-seated structures such as the superior temporal sulcus (STS) and the hippocampal formations, which are inaccessible to ECoG. This three-dimensinal coverage allows for a more holistic view of the neural networks involved in speech perception and production \cite{chang2015contemporary}. The high-gamma band activity captured by sEEG reflects the summation of action potentials from nearby neurons, providing a robust proxy for the intent to speak even in the absence of actual vocalization.

\section{Problem Statement and Objectives}
Despite significant progress, current speech synthesis systems from neural signals face several formidable challenges. Neural signals are inherently noisy and highly redundant. Furthermore, there is often a temporal misalignment between the neural activity and the resulting acoustic signal, caused both by the physiological delay in the motor system and the limitations of recording hardware.

Traditional decoding models, such as linear regressions or Forman-based Kalman filters, often fail to capture the complex, non-linear relationships between neural populations and the highly multidimensional acoustic space of human speech \cite{bib6, bib7}. While modern deep learning models have improved performance, many still struggle with inter-subject variability and the need for large datasets.

The objective of this research is to bridge these gaps through the introduction of the NeuroIncept Decoder. Our study focuses on three primary goals:
\begin{enumerate}
    \item Development of a robust multi-scale feature extraction pipeline using Inception modules to capture diverse temporal and spectral patterns in sEEG data.
    \item Integration of temporal modeling through Gated Recurrent Units (GRUs) to handle sequences and misalignments effectively.
    \item Validation of the system using a publicly available sEEG dataset across multiple participants to assess generalizability and accuracy in speech reconstruction.
\end{enumerate}

By combining these advanced architectural elements, we aim to produce a system capable of high-fidelity speech reconstruction that could eventually serve as the core of a real-time communication device for the severely speech-impaired.

\chapter{Methodology}
The methodology of this study is built upon a sophisticated pipeline designed to translate high-dimensional, noisy neural signals into intelligible acoustic representations. This chapter details the characteristics of the stereotactic EEG (sEEG) dataset, the signal processing techniques employed to extract meaningful features, and the internal architecture of the NeuroIncept Decoder.

\section{Dataset Description} \label{ssec:dataset}
The empirical foundation of this study is a high-resolution, publicly available dataset consisting of intracranial recordings from participants undergoing clinical monitoring for pharmacoresistant epilepsy \cite{verwoert2022dataset}. The dataset includes data from 10 Dutch participants (5 male, 5 female) with an average age of 32 years.

\subsection{Electrode Implantation and Monitoring}
Stereotactic EEG (sEEG) involves the implantation of depth electrodes—thin, flexible leads with multiple recording contacts—along a pre-planned trajectory. In this dataset, the placement of electrodes was determined solely by the clinical requirements of each participant’s epilepsy treatment. However, the trajectories frequently spanned key regions associated with the language network, including the superior temporal gyrus (STG), middle temporal gyrus (MTG), and Broca's area.

Each participant was implanted with between 6 and 14 depth electrodes, resulting in a vary number of individual recording channels (ranging from 64 to 128 per subject). The electrodes typically have a diameter of 0.8 mm, with 2 mm long contacts spaced 1.5 mm apart. This high-density contact arrangement allows for the recording of precise local field potentials (LFPs) with minimal volume conduction from distant brain regions.

\subsection{Experimental Paradigm: The Dutch IFA Corpus}
During the recording sessions, participants read aloud a curated list of words from the Dutch Institute of Functional and Anatomical Sciences (IFA) corpus \cite{ifaCorpus}. The task consisted of producing 100 isolated words, selected to cover a diverse range of phonemes and articulating movements. 

The simultaneous recording of sEEG and acoustic signals was achieved with sub-millisecond synchronization. Neural signals were sampled at either 1024 Hz or 2048 Hz using clinical-grade amplifiers. The speech signals were captured using a high-fidelity microphone at a sampling rate of 48 kHz. For the purpose of our decoding model, neural data were standardized to a 1024 Hz sampling rate, while audio signals were down-sampled to 16 kHz to reduce computational complexity while preserving speech intelligibility. Pitch modulation was applied to the audio using the LibROSA library \cite{librosa} to ensure participant anonymity, a standard ethical requirement for public neural datasets.

\section{Signal Processing}
The goal of the signal processing pipeline is to extract the high-gamma band activity from the sEEG recordings and convert the speech signals into a representation that is compatible with neural decoding.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{../Biblo/dataprocessing.jpg}
    \caption{Preprocessing pipeline for the sEEG and audio signals, showing the transition from raw neural spikes to analytic envelopes.}
    \label{fig:dataprocessing}
\end{figure}

\subsection{Neural Feature Extraction: The High-Gamma Band}
The high-gamma band (70-170 Hz) has been identified as a robust proxy for local multi-unit activity and is highly correlated with both sensory processing and motor execution \cite{band1}. To isolate this band, the raw sEEG signals underwent several stages of filtering:
\begin{enumerate}
    \item \textbf{High-Pass Filtering}: A zero-phase Butterworth filter was applied with a cutoff of 0.5 Hz to remove DC offsets and slow-wave artifacts.
    \item \textbf{Notch Filtering}: To eliminate power line interference, notch filters were applied at 50 Hz and its harmonics (100 Hz, 150 Hz).
    \item \textbf{Band-Pass Filtering}: The high-gamma components were isolated using a 70-170 Hz bandpass filter.
\end{enumerate}

After filtering, the Hilbert transform was utilized to compute the analytic signal $z(t) = a(t) + i\hat{a}(t)$, where $\hat{a}(t)$ is the Hilbert transform of the filtered LFP. The instantaneous amplitude (the envelope) was then calculated:
\begin{equation}
    E(t) = \sqrt{a(t)^2 + \hat{a}(t)^2}
\end{equation}
This envelope captures the power fluctuations within the high-gamma band. The envelope signals were then segmented into 50 ms temporal windows with a 10 ms frame shift, yielding a feature matrix for each participant.

\subsection{Acoustic Processing: Log-Mel Spectrograms}
To represent the target speech, we utilized log-Mel spectrograms, which are widely used in speech synthesis and recognition due to their alignment with human auditory perception. The conversion process involves:
\begin{enumerate}
    \item \textbf{Short-Time Fourier Transform (STFT)}: Computed on 50 ms windows with a 10 ms shift using a Hanning window.
    \item \textbf{Mel Filter-Bank}: The linear frequency scale was mapped to the non-linear Mel scale using 128 triangular filters, focusing on the frequencies most relevant to human speech (0-8000 Hz).
    \item \textbf{Logarithmic Scaling}: Applied to the Mel amplitudes to compress the dynamic range, resulting in the final log-Mel spectrograms.
\end{enumerate}

\section{Decoding Model Architecture} \label{ssec:neuroincept}
The NeuroIncept Decoder is a hybrid deep learning architecture designed to handle the unique challenges of neural decoding: high dimensionality, non-linearity, and temporal dependencies.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{../Biblo/model.png}
    \caption{Detailed architecture of the NeuroIncept Decoder, illustrating the Inception and Recurrent modules.}
    \label{fig:model}
\end{figure}

\subsection{Multi-Scale Feature Extraction: The Inception Module}
At the core of the NeuroIncept architecture is the Inception module, originally developed for computer vision but adapted here for time-series analysis \cite{inception}. Speech-related neural patterns occur at multiple time scales—some movements are rapid (e.g., plosives), while others are more sustained (e.g., vowels).

The Inception module processes the input through four parallel branches:
\begin{itemize}
    \item \textbf{Branch 1}: 1x1 convolutions for dimensionality reduction and cross-channel feature integration.
    \item \textbf{Branch 2}: 1x1 followed by 3x3 convolutions to capture localized temporal features.
    \item \textbf{Branch 3}: 1x1 followed by 5x5 convolutions to capture broader temporal patterns.
    \item \textbf{Branch 4}: MaxPooling followed by a 1x1 convolution for shift-invariance.
\end{itemize}
The outputs from these four branches are concatenated, allowing the model to "choose" the most relevant scale for each neural feature automatically.

\subsection{Temporal Modeling: The Recurrent Module}
Following the multi-scale feature extraction, the Recurrent module employs Gated Recurrent Units (GRUs) to model the sequential dependencies in the decoded patterns \cite{gru}. GRUs are chosen over traditional LSTMs due to their reduced parameter count and comparable performance on sequences of moderate length.

The module consists of three stacked GRU layers with decreasing units (512, 256, 128). Each unit utilizes update and reset gates to manage the hidden state, effectively learning which parts of the neural history are relevant for the current time step of synthesized speech. This recurrent structure is vital for mitigating the effects of temporal jitter and misalignment between recording modalities.

\subsection{Output and Optimization}
The processed features are finally passed through a series of dense (fully connected) layers with dropout (p=0.3) to prevent overfitting. The final layer produces a vector of 128 units, corresponding to one frame of the log-Mel spectrogram. The model is trained using the Mean Squared Error (MSE) loss function, optimized via Adam with an initial learning rate of $10^{-4}$ and an early stopping criterion based on validation loss.


\chapter{Results and Discussion}
The performance of the NeuroIncept Decoder was rigorously evaluated through a series of experiments focusing on reconstruction accuracy, stability across subjects, and qualitative analysis of the synthesized spectrograms. This chapter details our findings and provides a comparative perspective against baseline models.

\section{Subject-Specific Performance Analysis} \label{ssec_detailed_results}
Consistent with the high dimensionality and variability of neural signals, we observed distinct performance patterns across the 10 participants. Table \ref{tab:participants} summarizes the core metrics: Mean Squared Error (MSE), Pearson Correlation Coefficient (PCC), and the Spectro-Temporal Glimpsing Index (STGI) \cite{edraki2022spectro}.

\begin{table}[htbp]
    \centering
    \footnotesize
    \caption{Performance metrics on individual subjects (Mean $\pm$ SD).}
    \begin{tabular}{|c|c|cc|cc|}
    \hline
    \textbf{Participant} & \textbf{MSE} & \multicolumn{2}{c|}{\textbf{PCC}} & \multicolumn{2}{c|}{\textbf{STGI}} \\ \hline
    & & \textbf{Value} & \textbf{STD} & \textbf{Value} & \textbf{STD} \\ \hline
    sub-01 & 0.445 & 0.921 & 0.003 & 0.511 & 0.004 \\ \hline
    sub-02 & 0.511 & 0.926 & 0.002 & 0.477 & 0.005 \\ \hline
    sub-03 & 0.506 & 0.925 & 0.002 & 0.502 & 0.005 \\ \hline
    sub-04 & 0.522 & 0.938 & 0.004 & 0.479 & 0.005 \\ \hline
    sub-05 & 0.594 & 0.932 & 0.003 & 0.502 & 0.003 \\ \hline
    sub-06 & 0.409 & 0.944 & 0.002 & 0.552 & 0.004 \\ \hline
    sub-07 & 0.788 & 0.942 & 0.004 & 0.511 & 0.006 \\ \hline
    sub-08 & 0.652 & 0.897 & 0.005 & 0.526 & 0.005 \\ \hline
    sub-09 & 0.400 & 0.917 & 0.002 & 0.459 & 0.004 \\ \hline
    sub-10 & 0.498 & 0.838 & 0.007 & 0.522 & 0.004 \\ \hline
    \end{tabular}
    \label{tab:participants}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../Biblo/losses.png}
        \caption{Training and validation loss curves across epochs.}
        \label{fig:losses}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../Biblo/correlations.png}
        \caption{Distribution of Pearson Correlation Coefficients (PCC) across subjects.}
        \label{fig:correlations}
    \end{minipage}
\end{figure}

The primary observation from our results is the robust performance achieved across the majority of the cohort...

In contrast, participant sub-10 showed the lowest correlation (PCC = 0.838). Post-hoc analysis of the electrode locations for sub-10 revealed that the majority of depth contacts were located in hippocampal and amygdalar structures, which, while involved in memory and emotional processing, play a secondary role in the instantaneous control of the speech articulators. Despite this suboptimal coverage, the NeuroIncept Decoder was still able to extract relevant signals, demonstrating the architecture's ability to "find" sparse speech-related patterns in data from less-than-ideal anatomical locations.

The STGI metrics, which measure the preservation of spectro-temporal glimpses essential for speech intelligibility, ranged from 0.459 to 0.552. These values indicate that while our model successfully reconstructs the overall "envelope" and coarse spectral features of speech, there is still progress to be made in capturing the fine-grained formant transitions that differentiate high-confusability phonemes.

\section{Qualitative Insights into Synthesized Speech}
Beyond numerical metrics, a qualitative inspection of the predicted spectrograms reveals several key strengths of the NeuroIncept architecture. As shown in Figure \ref{fig:spectrogram}, the model accurately reproduces the onset and offset timings of vocalizations, effectively capturing the speech/silence rhythm.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{../Biblo/spectrogram_comparison.png}
    \caption{Qualitative comparison between original (top) and predicted (bottom) log-Mel spectrograms for a sample Dutch word.}
    \label{fig:spectrogram}
\end{figure}

The inclusion of the Inception module's 5x5 filters proved particularly effective at reconstructing long-duration phonemes, such as vowels and sonorants, which require temporal integration over longer windows. Meanwhile, the 3x3 filters successfully captured the transient spectral signatures of stops and fricatives. The GRU-based recurrent module ensured that these individual phonemic features were linked together with natural-looking transitions, avoiding the "jittery" or disconnected appearance common in models that treat each time-frame as an independent sample.

However, we noted that the model occasionally "smoothed" out the higher-frequency details above 4000 Hz, where the energy of fricatives like /s/ and /f/ is concentrated. This is a known challenge in MSE-based training, where the model tends to predict the mean value to minimize error, resulting in a loss of high-frequency "crispness." Future work using adversarial loss (GANs) could potentially address this blurring effect.

\section{Quantitative Model Comparison} \label{ssec:comparison_results}
To place our results in context, we compared the NeuroIncept Decoder against several state-of-the-art architectures from the literature. Table \ref{tab:comparison} presents this comparison.

\begin{table}[htbp]
    \centering
    \footnotesize
    \caption{Performance Comparison of NeuroIncept Decoder against Baselines}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Model Type}  & \textbf{Primary Architecture} & \textbf{Avg. PCC} & \textbf{Avg. STGI} \\ \hline
    Linear Model \cite{verwoert2022dataset} & Ordinary Least Squares & 0.705 & - \\ \hline
    FCN \cite{band1} & 3-Layer Fully Connected & 0.890 & 0.395 \\ \hline
    CNN \cite{band1} & 1D Convolutional & 0.898 & 0.484 \\ \hline
    LSTM-based \cite{bib9} & Recurrent (4 Layers) & 0.902 & 0.471 \\ \hline
    \textbf{NeuroIncept} & \textbf{Hybrid Inception-GRU} & \textbf{0.918} & \textbf{0.504} \\ \hline
    \end{tabular}
    \label{tab:comparison}
\end{table}

Our model achieved a statistically significant improvement (p < 0.01) over the baseline CNN and LSTM models. We hypothesize that this improvement stems from the dual-path nature of our architecture. While the LSTM-based models are excellent at temporal dependencies, they lack the multi-scale spatial filters necessary to deal with the spatial redundancy of sEEG contacts. Conversely, standard CNNs have a fixed receptive field that may not match the varying durations of phonetic units. By employing Inception modules, the NeuroIncept Decoder adaptively integrates features across multiple scales, providing a more versatile input to the recurrent layers.

\chapter{Conclusions}
This study represents a significant milestone in the ongoing quest to restore communication for individuals with severe neurological impairments. By leveraging invasive sEEG neural recordings and a novel deep learning architecture, we have demonstrated that high-fidelity speech reconstruction from brain activity is not only possible but increasingly accurate. This final chapter synthesizes our contributions, acknowledges the remaining hurdles, and outlines a roadmap for future research.

\section{Summary of Contributions}
The primary contribution of this work is the development and validation of the NeuroIncept Decoder. Our research has led to several key findings:
\begin{enumerate}
    \item \textbf{High-Correlation Decoding}: We achieved Pearson Correlation Coefficients (PCC) of up to 0.94 in individual participants, significantly outperforming traditional linear models and standard convolutional architectures.
    \item \textbf{Multi-Scale Feature Learning}: We demonstrated that the parallel convolutional structure of Inception modules is uniquely suited to the multi-scale nature of neural signals, allowing for the simultaneous capture of transient phonetic features and sustained prosodic patterns.
    \item \textbf{Robust Temporal Modeling}: The integration of GRUs allowed the system to maintain a coherent speech rhythm, resulting in synthesized spectrograms that exhibit natural-looking temporal transitions between phonetic units.
    \item \textbf{Anatomy-Performance Correlation}: Our detailed analysis of electrode placement confirmed that while peri-Sylvian coverage is ideal, deep-seated sEEG contacts in the temporal lobe can still provide valuable signals for speech synthesis, expanding the potential clinical utility of the system.
\end{enumerate}

\section{Current Limitations}
Despite the promising results, several technical and physiological challenges remain before such a system can be deployed in a clinical setting. 

First, the current model requires a significant amount of paired neural and acoustic data for each participant. In many clinical scenarios, particularly for individuals who are already "locked-in," collecting high-quality audio recordings for training is impossible. This necessitates the development of zero-shot or cross-subject decoding strategies.

Second, the signal-to-noise ratio (SNR) of neural signals remains a bottle-neck. While sEEG is superior to EEG, it is still susceptible to physiological artifacts (e.g., eye blinks, muscle activity) and environmental electrical noise. Current pre-processing techniques, although effective, may inadvertently remove subtle neural signatures that could enhance decoding precision.

Finally, the STGI values, while improved, indicate that the fine-grained spectral details required for perfect speech intelligibility are not yet fully captured. The resulting speech, when passed through a vocoder, may sound "robotic" or lack the specific identity (pitch and timbre) of the participant’s original voice.

\section{Future Directions: A Roadmap for Speech Restoration}
The next phase of this research will focus on three key pillars: real-time implementation, advanced neural vocoders, and cross-modal pre-training.

\subsection{Real-Time Decoding and Low Latency}
Translating our batch-processing model into a real-time system is a critical priority. This involves optimizing the NeuroIncept architecture for low-latency inference, potentially through model quantization or the use of specialized hardware such as Edge-TPUs. Our goal is to achieve an "end-to-end" latency of less than 100 ms, which is the threshold required for a user to perceive the synthesized speech as their own voice in real-time.

\subsection{Neural Vocoders and High-Fidelity Synthesis}
Instead of predicting log-Mel spectrograms, future iterations of the NeuroIncept architecture will explore the direct prediction of latent features for high-fidelity neural vocoders like WaveNet or HiFi-GAN. These generative models can reconstruct speech from compressed representations with much higher clarity and naturalness than traditional Griffin-Lim or Phase-Vocoder algorithms.

\subsection{Cross-Modal and Transfer Learning}
To address the lack of training data in some patients, we will investigate transfer learning techniques. By pre-training the NeuroIncept Decoder on large, multi-subject EEG or ECoG datasets, we may be able to "condition" the model on the general language network and then fine-tune it with only a few minutes of new data from a specific participant. Furthermore, integrating visual information (e.g., lip movements) during training could provide an additional "supervisory" signal to refine the neural decoding process.

In conclusion, while the path to a fully functional, real-time "speech prosthesis" is long, the results presented here provide a strong foundation for the belief that the barriers of silence can be broken through the synergy of neuroscience and artificial intelligence.

\backmatter
\appendix
\chapter{Supplementary Data}
This appendix contains additional technical details, hyperparameter tables for the NeuroIncept model, and high-resolution plots of original vs. predicted spectrograms for each participant.

\bibliographystyle{plain}
\bibliography{book_refs}

\end{document}
